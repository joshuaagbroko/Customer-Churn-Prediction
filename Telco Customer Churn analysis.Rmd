---
title: "Churn Classification Analysis"
author: "Joshua Agbroko"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview
A fictional telco company that provided home phone and internet services to 7043
customers in California in Q3.

## Data Description

Data Description

7043 observations with 33 variables

CustomerID: A unique ID that identifies each customer.

Count: A value used in reporting/dashboarding to sum up the number of customers 
in a filtered set.

Country: The country of the customer’s primary residence.

State: The state of the customer’s primary residence.

City: The city of the customer’s primary residence.

Zip Code: The zip code of the customer’s primary residence.

Lat Long: The combined latitude and longitude of the customer’s primary residence.

Latitude: The latitude of the customer’s primary residence.

Longitude: The longitude of the customer’s primary residence.

Gender: The customer’s gender: Male, Female

Senior Citizen: Indicates if the customer is 65 or older: Yes, No

Partner: Indicate if the customer has a partner: Yes, No

Dependents: Indicates if the customer lives with any dependents: Yes, No. 
Dependents could be children, parents, grandparents, etc.

Tenure Months: Indicates the total amount of months that the customer has been 
with the company by the end of the quarter specified above.

Phone Service: Indicates if the customer subscribes to home phone service with 
the company: Yes, No

Multiple Lines: Indicates if the customer subscribes to multiple telephone lines
with the company: Yes, No

Internet Service: Indicates if the customer subscribes to Internet service with 
the company: No, DSL, Fiber Optic, Cable.

Online Security: Indicates if the customer subscribes to an additional online 
security service provided by the company: Yes, No

Online Backup: Indicates if the customer subscribes to an additional online 
backup service provided by the company: Yes, No

Device Protection: Indicates if the customer subscribes to an additional device 
protection plan for their Internet equipment provided by the company: Yes, No

Tech Support: Indicates if the customer subscribes to an additional technical 
support plan from the company with reduced wait times: Yes, No

Streaming TV: Indicates if the customer uses their Internet service to stream 
television programming from a third party provider: Yes, No. The company does 
not charge an additional fee for this service.

Streaming Movies: Indicates if the customer uses their Internet service to 
stream movies from a third party provider: Yes, No. The company does not charge 
an additional fee for this service.

Contract: Indicates the customer’s current contract type: Month-to-Month, One 
Year, Two Year.

Paperless Billing: Indicates if the customer has chosen paperless billing: Yes, No

Payment Method: Indicates how the customer pays their bill: Bank Withdrawal, 
Credit Card, Mailed Check

Monthly Charge: Indicates the customer’s current total monthly charge for all 
their services from the company.

Total Charges: Indicates the customer’s total charges, calculated to the end of 
the quarter specified above.

Churn Label: Yes = the customer left the company this quarter. No = the customer
remained with the company. Directly related to Churn Value.

Churn Value: 1 = the customer left the company this quarter. 0 = the customer 
remained with the company. Directly related to Churn Label.

Churn Score: A value from 0-100 that is calculated using the predictive tool 
IBM SPSS Modeler. The model incorporates multiple factors known to cause churn. 
The higher the score, the more likely the customer will churn.

CLTV: Customer Lifetime Value. A predicted CLTV is calculated using corporate 
formulas and existing data. The higher the value, the more valuable the customer. 

Churn Reason: A customer’s specific reason for leaving the company. Directly 
related to Churn Category.


## Data Collection
```{r message=FALSE, warning=FALSE}
# Loading the required libraries
library(tidyverse)
library(skimr)
library(janitor)
library(caret)
library(readxl)
library(ggplot2)
library(car)
library(pROC)

df <- read_excel("Telco_customer_churn.xlsx", sheet = "Telco_Churn")

head(df)

```
## Data Preparation & Cleaning

```{r}
# Checking the structure
str(df)
```
```{r}
# Checking for missing values
colSums(is.na(df))
```
```{r}
# Converting categorical variables to factors
categorical_cols <- c("Gender", "Senior Citizen", "Partner", "Dependents", 
                      "Phone Service", "Multiple Lines", "Internet Service", 
                      "Online Backup", "Device Protection", "Tech Support",
                      "Streaming TV", "Streaming Movies", "Contract", 
                      "Paperless Billing", "Payment Method", "Churn Label")

df <- df %>% 
  mutate(across(all_of(categorical_cols), as.factor))

# Converting 'Total_Charges to numeric
df$`Total Charges` <- as.numeric(df$`Total Charges`)

# replacing NA values in 'Total_Charges' with 0
df$`Total Charges`[is.na(df$`Total Charges`)] <- 0

summary(df$`Total Charges`)
```

```{r}
str(df)
```
```{r}
# Replacing NA values in 'Churn Reason' with 'No Reason
df$`Churn Reason`[is.na(df$`Churn Reason`)] <- "No Reason"

# Converting to factor
df$`Churn Reason` <- as.factor(df$`Churn Reason`)

# Checking
summary(df$`Churn Reason`)
```

# EXploratory Data Analysis
```{r}
# Summary statistics
skim(df)

```

## Churn distribution
```{r}
df %>%
  count(`Churn Label`) %>%
  ggplot(aes(x = `Churn Label`, y = n, fill = `Churn Label`)) +
  geom_bar(stat = "Identity") + 
  theme_grey() +
  labs(title = "Customer Churn Distribution", x = "Churn Label", y ="Count")
  

```

### Churn vs Monthly Charges
```{r}
ggplot(df, aes(x = `Churn Label`, y =`Monthly Charges`, fill = `Churn Label`)) +
  geom_boxplot() +
  theme_grey() +
  labs(title = "Monthly Charges by Churn Status", x = "Churn Label",
       y = "Monthly Charges")
```

### Tenure distributiion
```{r}
ggplot(df, aes(x =`Tenure Months`, fill = `Churn Label`)) +
  geom_histogram(bins = 30, alpha = 0.7, position = "Identity") +
  theme_grey() +
  labs(title = "Tenure Distribution by Churn Status", x = "Tenure(Months)",
       y = "Count")

```
 ### Churn vs Contract Type
```{r}
df %>%
  count(Contract, `Churn Label`) %>%
  ggplot(aes(x = Contract, y = n, fill =`Churn Label`)) +
  geom_bar(stat = "Identity", position = "dodge") +
  theme_grey() +
  labs(title = "Churn Rate by Contract Type", x = "Contract Type", y = "Count")
```

# Feature Selection & Logistic Regression Modelling 
### Feature Selection
```{r}
# Removing redundant columns
df <- df %>%
  select(-c(CustomerID, `Lat Long`, `Zip Code`, Country, State, City))

# Converting categorical variables to dummy variables
df <- df %>%
  mutate(across(where(is.factor), as.numeric))
```

```{r}
# Check for multicollinearity using VIF
# Identify columns with zero variance
constant_columns <- df %>% select_if(is.numeric) %>% summarise_all(var) %>% 
  gather(key = "col", value = "variance") %>%
  filter(variance == 0) %>%
  pull(col)

# Remove these columns
df <- df %>% select(-all_of(constant_columns))
# Compute correlation matrix after removing constant columns
cor_matrix <- cor(df %>% select_if(is.numeric), use = "pairwise.complete.obs")

# Replace NA values with 0 (optional, if necessary)
cor_matrix[is.na(cor_matrix)] <- 0
# Find highly correlated features (correlation > 0.75)
highly_correlated <- findCorrelation(cor_matrix, cutoff = 0.75, verbose = TRUE)

# Drop these columns
df <- df %>% select(-highly_correlated)
# Fit logistic regression model
log_model <- glm(`Churn Value` ~ ., data = df, family = binomial)


# Compute VIF
vif_data <- vif(log_model)

# Print VIF score
print(vif_data)

# Removing highly correlated features (VIF > 5)
df <- df %>%
  select(-one_of(names(vif_data[vif_data > 5])))

# Check
str(df)

```

### Train-Test Split
```{r}
set.seed(123)
trainIndex <- createDataPartition(df$`Churn Value`, p = 0.8, list = FALSE)
trainData <- df[trainIndex, ]
testData <- df[-trainIndex, ]

```
### Model Training
```{r}
# Train Logistic model
model <- glm(`Churn Value` ~ ., data = trainData, family = binomial)

# Model summary
summary(model)

```

```{r Making Predictions}
pred_probs <- predict(log_model, testData, type = "response")

# Converting probabilities to class labels
pred_labels <- ifelse(pred_probs > 0.5, 1, 0)

# Converting to factor for evaluation
pred_labels <- as.factor(pred_labels)
testData$`Churn Value` <- as.factor(testData$`Churn Value`)

```


### Evaluating Model Perfomance
```{r}
# confusion Matrix
confusionMatrix(pred_labels, testData$`Churn Value`)

# Computing accuracy
accuracy <- mean(pred_labels == testData$`Churn Value`)
print(paste("Model Accuracy", round(accuracy * 100, 2), "%"))


```

### Cross-Validation
Performing cross-validation to ensure the model generalizes well. Applying 
10-fold cross-validation.
```{r}
# 10-fold CV
cv_model <- train(`Churn Value` ~ ., data = trainData, method = "glm",
                  family = binomial,
                  trControl = trainControl(method = "cv", number = 10))
cv_model
```

```{r}

library(glmnet)
# Convert to matrix for glmnet
x_train <- model.matrix(`Churn Value` ~ ., trainData)[, -1]
y_train <- trainData$`Churn Value`

# Train LASSO model
lasso_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)

# Features selected by LASSO
selected_features <- coef(lasso_model, s = "lambda.min")
selected_features <- as.matrix(selected_features)
selected_features <- selected_features[selected_features[, 1] != 0, , drop = FALSE]

print(selected_features)

```

#### Training Logisic regression model with Lasso Selected features
```{r}
# Remove backticks and specific category labels
clean_selected_features <- gsub("`", "", selected_features)  # Remove backticks
clean_selected_features <- gsub("No internet service|Yes|No", "", 
                                clean_selected_features)  # Remove specific category labels
clean_selected_features <- trimws(clean_selected_features)  

# Find actual features that exist in trainData
selected_features_final <- intersect(clean_selected_features, names(trainData))

important_features <- c("Churn Score", "Churn Reason", "Contract", 
                        "Tenure Months", "Paperless Billing")

selected_features_final <- unique(c(selected_features_final, important_features))

trainData_lasso <- trainData %>%
  select(all_of(selected_features_final), `Churn Value`)


# Train logistic regression
log_model_lasso <- glm(`Churn Value` ~ ., data = trainData_lasso, 
                       family = binomial)
summary(log_model_lasso)
```
# Model Evaluation
```{r}
predictions <- predict(log_model_lasso, trainData_lasso, type = "response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
confusionMatrix(as.factor(predicted_classes), as.factor(trainData_lasso$`Churn Value`))
```

```{r}
roc_curve <- roc(trainData_lasso$`Churn Value`, predictions)
plot(roc_curve, col="blue")
auc(roc_curve)
```

```{r eval=FALSE, include=FALSE}
## Processed Customer data
write.csv(trainData, "customer_churn_data.csv", row.names = FALSE)

## Model predictions
trainData$predicted_churn <- predict(log_model_lasso, 
                                     trainData_lasso,
                                     type = "response")
write.csv(trainData[ , c("Churn Value", "predicted_churn")], "churn_predictions.csv",
          row.names = FALSE)

## Feature Importance
# Extract coefficients as a data frame
feature_importance <- as.data.frame(as.matrix(coef(log_model_lasso)))

feature_importance$Feature <- rownames(feature_importance)

colnames(feature_importance) <- c("Coefficient", "Feature")

feature_importance <- feature_importance[, c("Feature", "Coefficient")]

write.csv(feature_importance, "feature_importance.csv", row.names = FALSE)

```


# Conclusion
In this analysis, we built a logistic regression model to predict customer churn. 
The model achieved an accuracy of 94.01%, with a sensitivity of 96.86% 
(detecting non-churners) and specificity of 86.69% (detecting churners).

While the model performed quite well, there was a slight imbalance in 
misclassifications, indicating a tendency to 
predict non-churn (0) more often. To improve churn detection, adjusting the 
decision threshold or using SMOTE could help.
